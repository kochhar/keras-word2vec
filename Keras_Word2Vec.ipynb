{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras-Word2Vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kochhar/keras-word2vec/blob/master/Keras_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "m_rbaQARKBDg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "4d23a2f4-c53f-4e66-9645-0a23276de4c6"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://kochhar@github.com/kochhar/keras-word2vec"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-word2vec'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/45)   \u001b[K\rremote: Counting objects:   4% (2/45)   \u001b[K\rremote: Counting objects:   6% (3/45)   \u001b[K\rremote: Counting objects:   8% (4/45)   \u001b[K\rremote: Counting objects:  11% (5/45)   \u001b[K\rremote: Counting objects:  13% (6/45)   \u001b[K\rremote: Counting objects:  15% (7/45)   \u001b[K\rremote: Counting objects:  17% (8/45)   \u001b[K\rremote: Counting objects:  20% (9/45)   \u001b[K\rremote: Counting objects:  22% (10/45)   \u001b[K\rremote: Counting objects:  24% (11/45)   \u001b[K\rremote: Counting objects:  26% (12/45)   \u001b[K\rremote: Counting objects:  28% (13/45)   \u001b[K\rremote: Counting objects:  31% (14/45)   \u001b[K\rremote: Counting objects:  33% (15/45)   \u001b[K\rremote: Counting objects:  35% (16/45)   \u001b[K\rremote: Counting objects:  37% (17/45)   \u001b[K\rremote: Counting objects:  40% (18/45)   \u001b[K\rremote: Counting objects:  42% (19/45)   \u001b[K\rremote: Counting objects:  44% (20/45)   \u001b[K\rremote: Counting objects:  46% (21/45)   \u001b[K\rremote: Counting objects:  48% (22/45)   \u001b[K\rremote: Counting objects:  51% (23/45)   \u001b[K\rremote: Counting objects:  53% (24/45)   \u001b[K\rremote: Counting objects:  55% (25/45)   \u001b[K\rremote: Counting objects:  57% (26/45)   \u001b[K\rremote: Counting objects:  60% (27/45)   \u001b[K\rremote: Counting objects:  62% (28/45)   \u001b[K\rremote: Counting objects:  64% (29/45)   \u001b[K\rremote: Counting objects:  66% (30/45)   \u001b[K\rremote: Counting objects:  68% (31/45)   \u001b[K\rremote: Counting objects:  71% (32/45)   \u001b[K\rremote: Counting objects:  73% (33/45)   \u001b[K\rremote: Counting objects:  75% (34/45)   \u001b[K\rremote: Counting objects:  77% (35/45)   \u001b[K\rremote: Counting objects:  80% (36/45)   \u001b[K\rremote: Counting objects:  82% (37/45)   \u001b[K\rremote: Counting objects:  84% (38/45)   \u001b[K\rremote: Counting objects:  86% (39/45)   \u001b[K\rremote: Counting objects:  88% (40/45)   \u001b[K\rremote: Counting objects:  91% (41/45)   \u001b[K\rremote: Counting objects:  93% (42/45)   \u001b[K\rremote: Counting objects:  95% (43/45)   \u001b[K\rremote: Counting objects:  97% (44/45)   \u001b[K\rremote: Counting objects: 100% (45/45)   \u001b[K\rremote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects:   4% (1/25)   \u001b[K\rremote: Compressing objects:   8% (2/25)   \u001b[K\rremote: Compressing objects:  12% (3/25)   \u001b[K\rremote: Compressing objects:  16% (4/25)   \u001b[K\rremote: Compressing objects:  20% (5/25)   \u001b[K\rremote: Compressing objects:  24% (6/25)   \u001b[K\rremote: Compressing objects:  28% (7/25)   \u001b[K\rremote: Compressing objects:  32% (8/25)   \u001b[K\rremote: Compressing objects:  36% (9/25)   \u001b[K\rremote: Compressing objects:  40% (10/25)   \u001b[K\rremote: Compressing objects:  44% (11/25)   \u001b[K\rremote: Compressing objects:  48% (12/25)   \u001b[K\rremote: Compressing objects:  52% (13/25)   \u001b[K\rremote: Compressing objects:  56% (14/25)   \u001b[K\rremote: Compressing objects:  60% (15/25)   \u001b[K\rremote: Compressing objects:  64% (16/25)   \u001b[K\rremote: Compressing objects:  68% (17/25)   \u001b[K\rremote: Compressing objects:  72% (18/25)   \u001b[K\rremote: Compressing objects:  76% (19/25)   \u001b[K\rremote: Compressing objects:  80% (20/25)   \u001b[K\rremote: Compressing objects:  84% (21/25)   \u001b[K\rremote: Compressing objects:  88% (22/25)   \u001b[K\rremote: Compressing objects:  92% (23/25)   \u001b[K\rremote: Compressing objects:  96% (24/25)   \u001b[K\rremote: Compressing objects: 100% (25/25)   \u001b[K\rremote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 45 (delta 17), reused 36 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nBgfNh41h6Lh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41cd1789-8985-4c68-c3e3-d882870de379"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/keras-word2vec')\n",
        "os.getcwd()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/keras-word2vec'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "PQ14GMrGm-IM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "41172efd-2127-447a-d0b6-32c0a8022068"
      },
      "cell_type": "code",
      "source": [
        "!git remote set-url origin https://kochhar@github.com/kochhar/keras-word2vec\n",
        "!git pull"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects:  25% (1/4)   \u001b[K\rremote: Counting objects:  50% (2/4)   \u001b[K\rremote: Counting objects:  75% (3/4)   \u001b[K\rremote: Counting objects: 100% (4/4)   \u001b[K\rremote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects:  33% (1/3)   \u001b[K\rremote: Compressing objects:  66% (2/3)   \u001b[K\rremote: Compressing objects: 100% (3/3)   \u001b[K\rremote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  33% (1/3)   \rUnpacking objects:  66% (2/3)   \rUnpacking objects: 100% (3/3)   \rUnpacking objects: 100% (3/3), done.\n",
            "From https://github.com/kochhar/keras-word2vec\n",
            "   13fe8f4..f96f1ea  master     -> origin/master\n",
            "hint: Waiting for your editor to close the file... fatal: cannot run editor: No such file or directory\n",
            "error: unable to start editor 'editor'\n",
            "Not committing merge; use 'git commit' to complete the merge.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IQ6x01cQh-Y2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dda05ce3-9efe-435c-ccb5-55c67643d22f"
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import logging\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import word2vec as w2v\n",
        "from importlib import reload\n",
        "w2v = reload(w2v)\n",
        "\n",
        "logging.basicConfig(format=\"[%(asctime)s]%(levelname)s:%(name)s:%(message)s\",\n",
        "                    level=logging.INFO)\n",
        "logging.info('logging configured')\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2018-10-30 03:26:30,161]INFO:root:logging configured\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "R2Ngk61PiH7I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "91b665bf-12cc-45bf-c977-b23f95e7b1f2"
      },
      "cell_type": "code",
      "source": [
        "filename = 'data/text8.zip'\n",
        "logging.info('Reading corpus')\n",
        "training_corpus = w2v.str_from_zipfile(filename).split()\n",
        "logging.info('Corpus: %s words', len(training_corpus))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2018-10-30 03:07:14,666]INFO:root:Reading corpus\n",
            "[2018-10-30 03:07:16,853]INFO:root:Corpus: 17005207 words\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YJqHZUKcih6x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "dc6d23ec-db33-41c4-962c-b39d0cd2dc28"
      },
      "cell_type": "code",
      "source": [
        "logging.info('Creating vocabulary')\n",
        "counts = [('UNK', -1)] + collections.Counter(training_corpus).most_common(int(1e4-1))\n",
        "vocab = w2v.Vocabulary([word for (word, _) in counts ])\n",
        "logging.info('Encoding corpus')\n",
        "tokens = vocab.encode(training_corpus)\n",
        "logging.info(\"Vocab: %s tokens, Sample: %s\", vocab.size, list(zip(training_corpus[:7], tokens[:7])))\n",
        "del training_corpus\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2018-10-30 03:07:20,751]INFO:root:Creating vocabulary\n",
            "[2018-10-30 03:07:24,523]INFO:root:Encoding corpus\n",
            "[2018-10-30 03:07:27,941]INFO:root:Vocab: 10000 tokens, Sample: [('anarchism', 5234), ('originated', 3081), ('as', 12), ('a', 6), ('term', 195), ('of', 2), ('abuse', 3134)]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "EpZLaYIfjut9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5a526517-a8c5-405f-9a70-3094d33c88af"
      },
      "cell_type": "code",
      "source": [
        "logging.info('Creating skipgrams')\n",
        "word_target, word_context, labels = sgns.skipgrams(tokens, window_size=3)\n",
        "logging.info('Training skipgrams: %s, Sample: %s', len(word_target), \n",
        "             list(zip(word_target[:10], word_context[:10], labels[:10])))\n",
        "del tokens"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2018-10-30 03:07:38,411]INFO:root:Creating skipgrams\n",
            "[2018-10-30 03:12:05,531]INFO:root:Training skipgrams: 30022202, Sample: [(9817, 3, 1), (1460, 1264, 0), (6786, 831, 0), (6777, 7884, 0), (278, 990, 0), (606, 6433, 0), (5230, 8627, 0), (669, 9626, 0), (1057, 1614, 0), (2831, 4311, 0)]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "sfCPTr2EBm71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Similarity(object):\n",
        "    def __init__(self, sim_model):\n",
        "        \"\"\"Params:\n",
        "            - sim_model: A keras model which outputs a similarity score given\n",
        "              two vectors as inputs\"\"\"\n",
        "        self.sim_model = sim_model\n",
        "\n",
        "    def most_similar(self, examples, vocab, top_k=10):\n",
        "        sim_fn = self.sim\n",
        "        contexts = range(vocab.size)\n",
        "        # compute the similarity between the example and every context word in the vocabulary\n",
        "        sims = [(ex, sim_fn(ex, contexts)) for ex in examples]\n",
        "        sims = [(ex, (-sim).argsort()[1:top_k+1], sim) for (ex, sim) in sims]\n",
        "        return [(ex, top_idx, sim[top_idx]) for (ex, top_idx, sim) in sims]\n",
        "\n",
        "    def sim(self, target, contexts):\n",
        "        context_b = np.array(contexts)\n",
        "        # the target word for every item in the batch is the same\n",
        "        target_b = np.array([target]*len(contexts))\n",
        "        sim_b = self.sim_model.predict_on_batch([target_b, context_b])\n",
        "        \n",
        "        # reshape from (len(contexts), 1) to (len(contexts),)\n",
        "        sim_b = sim_b.reshape(context_b.shape)\n",
        "        return sim_b\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPY4gYV1n4Nk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3892
        },
        "outputId": "63e5ddd1-808d-4d34-e798-b28e85f3cc29"
      },
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "sgns = w2v.SkipGramNegSample(vocab_size=vocab.size, embedding_dim=300)\n",
        "model, validation_model = sgns.model()\n",
        "sim_eval = Similarity(validation_model)\n",
        "\n",
        "# Validation set for inspecting training progress\n",
        "# Max index of the random sampling from tokens (implies choosing\n",
        "# from the the top-N occurring tokens)\n",
        "max_token_idx = 128\n",
        "valid_samples = np.random.choice(np.arange(24, max_token_idx), size=16, replace=False)\n",
        "\n",
        "# Feeding to the network\n",
        "batch_size = 128\n",
        "tgt_b, ctx_b, lbl_b = np.zeros((batch_size,)), np.zeros((batch_size,)), np.zeros((batch_size,))\n",
        "# training loop over 1e6 epochs with batches of size 32\n",
        "for epoch in range(6):\n",
        "  for idx in range(0, 1024000, batch_size):\n",
        "    offset = epoch * 1024000 + idx\n",
        "    tgt_b[0:batch_size,] = word_target[offset:offset+batch_size]\n",
        "    ctx_b[0:batch_size,] = word_context[offset:offset+batch_size]\n",
        "    lbl_b[0:batch_size,] = labels[offset:offset+batch_size]\n",
        "    loss = model.train_on_batch([tgt_b, ctx_b], lbl_b)\n",
        "\n",
        "    if idx % 51200 == 0:\n",
        "      logging.info(\"Epoch: %s, example@%s, loss=%s\", epoch, offset, loss)\n",
        "  \n",
        "  most_similar = sim_eval.most_similar(valid_samples, vocab)\n",
        "  log_str = '\\n'+'\\n'.join(['Most similar to {} (median: {:.5}): {}' .format(vocab.decode([ex]), scores[4], \n",
        "                                                                             ', '.join(vocab.decode(top_idx)))\n",
        "                           for (ex, top_idx, scores) in most_similar])\n",
        "  logging.info(log_str)\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2018-10-30 05:30:50,431]INFO:root:Epoch: 0, example@0, loss=0.6927639\n",
            "[2018-10-30 05:30:53,555]INFO:root:Epoch: 0, example@51200, loss=0.69472253\n",
            "[2018-10-30 05:30:56,677]INFO:root:Epoch: 0, example@102400, loss=0.69249415\n",
            "[2018-10-30 05:30:59,771]INFO:root:Epoch: 0, example@153600, loss=0.6925862\n",
            "[2018-10-30 05:31:02,869]INFO:root:Epoch: 0, example@204800, loss=0.6883968\n",
            "[2018-10-30 05:31:05,941]INFO:root:Epoch: 0, example@256000, loss=0.67755044\n",
            "[2018-10-30 05:31:09,035]INFO:root:Epoch: 0, example@307200, loss=0.65029216\n",
            "[2018-10-30 05:31:12,120]INFO:root:Epoch: 0, example@358400, loss=0.78567576\n",
            "[2018-10-30 05:31:15,182]INFO:root:Epoch: 0, example@409600, loss=0.58435905\n",
            "[2018-10-30 05:31:18,268]INFO:root:Epoch: 0, example@460800, loss=0.7329979\n",
            "[2018-10-30 05:31:21,354]INFO:root:Epoch: 0, example@512000, loss=0.57221025\n",
            "[2018-10-30 05:31:24,451]INFO:root:Epoch: 0, example@563200, loss=0.6042657\n",
            "[2018-10-30 05:31:27,555]INFO:root:Epoch: 0, example@614400, loss=0.5417385\n",
            "[2018-10-30 05:31:30,630]INFO:root:Epoch: 0, example@665600, loss=0.6076255\n",
            "[2018-10-30 05:31:33,708]INFO:root:Epoch: 0, example@716800, loss=0.52431786\n",
            "[2018-10-30 05:31:36,778]INFO:root:Epoch: 0, example@768000, loss=0.60240495\n",
            "[2018-10-30 05:31:39,807]INFO:root:Epoch: 0, example@819200, loss=0.5437836\n",
            "[2018-10-30 05:31:42,840]INFO:root:Epoch: 0, example@870400, loss=0.5110264\n",
            "[2018-10-30 05:31:45,902]INFO:root:Epoch: 0, example@921600, loss=0.6371134\n",
            "[2018-10-30 05:31:48,947]INFO:root:Epoch: 0, example@972800, loss=0.5148022\n",
            "[2018-10-30 05:31:52,325]INFO:root:\n",
            "Most similar to ['i'] (median: 0.60609): into, but, by, and, if, who, including, in, when, zero\n",
            "Most similar to ['may'] (median: 0.57689): can, john, but, into, including, in, i, and, for, british\n",
            "Most similar to ['made'] (median: 0.5953): for, but, or, into, john, zero, during, including, no, when\n",
            "Most similar to ['first'] (median: 0.44201): some, use, including, under, into, other, for, national, by, with\n",
            "Most similar to ['would'] (median: 0.46264): can, could, to, should, may, will, over, under, john, or\n",
            "Most similar to ['up'] (median: 0.48003): if, but, or, of, including, on, for, into, during, in\n",
            "Most similar to ['during'] (median: 0.6861): into, or, john, but, including, their, in, when, who, some\n",
            "Most similar to ['or'] (median: 0.70633): but, into, including, during, john, under, with, zero, for, of\n",
            "Most similar to ['history'] (median: 0.53687): into, including, john, use, but, with, some, in, or, british\n",
            "Most similar to ['so'] (median: 0.48386): had, has, into, was, john, between, including, or, during, were\n",
            "Most similar to ['had'] (median: 0.6004): were, has, into, during, of, or, was, john, including, have\n",
            "Most similar to ['no'] (median: 0.63131): into, his, an, when, her, but, use, this, their, by\n",
            "Most similar to ['m'] (median: 0.23404): away, prices, age, http, shaft, consistently, september, combining, rock, farms\n",
            "Most similar to ['will'] (median: 0.54137): can, or, including, john, but, may, has, during, into, are\n",
            "Most similar to ['being'] (median: 0.40202): but, including, was, may, many, has, will, from, john, time\n",
            "Most similar to ['since'] (median: 0.24294): company, changing, function, debate, frank, point, dynasty, michael, via, radio\n",
            "[2018-10-30 05:31:52,343]INFO:root:Epoch: 1, example@1024000, loss=0.558145\n",
            "[2018-10-30 05:31:55,386]INFO:root:Epoch: 1, example@1075200, loss=0.52496475\n",
            "[2018-10-30 05:31:58,440]INFO:root:Epoch: 1, example@1126400, loss=0.46321225\n",
            "[2018-10-30 05:32:01,505]INFO:root:Epoch: 1, example@1177600, loss=0.54733074\n",
            "[2018-10-30 05:32:04,558]INFO:root:Epoch: 1, example@1228800, loss=0.58947706\n",
            "[2018-10-30 05:32:07,601]INFO:root:Epoch: 1, example@1280000, loss=0.5261507\n",
            "[2018-10-30 05:32:10,651]INFO:root:Epoch: 1, example@1331200, loss=0.54005635\n",
            "[2018-10-30 05:32:13,693]INFO:root:Epoch: 1, example@1382400, loss=0.56978905\n",
            "[2018-10-30 05:32:16,742]INFO:root:Epoch: 1, example@1433600, loss=0.57123995\n",
            "[2018-10-30 05:32:19,782]INFO:root:Epoch: 1, example@1484800, loss=0.57044274\n",
            "[2018-10-30 05:32:22,843]INFO:root:Epoch: 1, example@1536000, loss=0.56496954\n",
            "[2018-10-30 05:32:25,902]INFO:root:Epoch: 1, example@1587200, loss=0.64055985\n",
            "[2018-10-30 05:32:28,943]INFO:root:Epoch: 1, example@1638400, loss=0.56069195\n",
            "[2018-10-30 05:32:31,996]INFO:root:Epoch: 1, example@1689600, loss=0.49113235\n",
            "[2018-10-30 05:32:35,039]INFO:root:Epoch: 1, example@1740800, loss=0.5270748\n",
            "[2018-10-30 05:32:38,103]INFO:root:Epoch: 1, example@1792000, loss=0.4603306\n",
            "[2018-10-30 05:32:41,146]INFO:root:Epoch: 1, example@1843200, loss=0.56121075\n",
            "[2018-10-30 05:32:44,194]INFO:root:Epoch: 1, example@1894400, loss=0.49797413\n",
            "[2018-10-30 05:32:47,252]INFO:root:Epoch: 1, example@1945600, loss=0.491496\n",
            "[2018-10-30 05:32:50,306]INFO:root:Epoch: 1, example@1996800, loss=0.5159772\n",
            "[2018-10-30 05:32:53,450]INFO:root:\n",
            "Most similar to ['i'] (median: 0.77887): or, under, into, when, from, these, who, if, over, including\n",
            "Most similar to ['may'] (median: 0.72482): would, can, will, under, or, including, into, over, for, had\n",
            "Most similar to ['made'] (median: 0.53553): often, also, for, between, with, which, under, from, or, over\n",
            "Most similar to ['first'] (median: 0.8111): from, under, at, other, between, when, with, through, by, most\n",
            "Most similar to ['would'] (median: 0.69034): can, may, will, to, or, under, over, for, including, into\n",
            "Most similar to ['up'] (median: 0.71209): from, if, on, including, under, or, through, against, for, had\n",
            "Most similar to ['during'] (median: 0.82605): or, under, at, into, from, including, in, when, after, through\n",
            "Most similar to ['or'] (median: 0.87509): under, including, into, from, these, however, de, after, for, all\n",
            "Most similar to ['history'] (median: 0.76321): many, most, some, under, including, all, several, both, or, when\n",
            "Most similar to ['so'] (median: 0.35733): between, new, had, other, both, are, such, political, has, be\n",
            "Most similar to ['had'] (median: 0.83674): were, or, including, has, from, have, under, into, new, other\n",
            "Most similar to ['no'] (median: 0.73116): an, his, its, this, many, her, a, several, their, de\n",
            "Most similar to ['m'] (median: 0.55038): strong, lawrence, september, white, blade, few, discovered, pianist, uk, mi\n",
            "Most similar to ['will'] (median: 0.7642): or, would, under, may, were, had, including, between, most, from\n",
            "Most similar to ['being'] (median: 0.65068): many, often, including, from, first, were, her, but, his, had\n",
            "Most similar to ['since'] (median: 0.57628): through, from, he, with, into, by, if, like, first, during\n",
            "[2018-10-30 05:32:53,466]INFO:root:Epoch: 2, example@2048000, loss=0.53309524\n",
            "[2018-10-30 05:32:56,500]INFO:root:Epoch: 2, example@2099200, loss=0.5529859\n",
            "[2018-10-30 05:32:59,657]INFO:root:Epoch: 2, example@2150400, loss=0.56637627\n",
            "[2018-10-30 05:33:02,830]INFO:root:Epoch: 2, example@2201600, loss=0.5734776\n",
            "[2018-10-30 05:33:05,995]INFO:root:Epoch: 2, example@2252800, loss=0.5358145\n",
            "[2018-10-30 05:33:09,186]INFO:root:Epoch: 2, example@2304000, loss=0.5180826\n",
            "[2018-10-30 05:33:12,396]INFO:root:Epoch: 2, example@2355200, loss=0.5541401\n",
            "[2018-10-30 05:33:15,487]INFO:root:Epoch: 2, example@2406400, loss=0.5599281\n",
            "[2018-10-30 05:33:18,516]INFO:root:Epoch: 2, example@2457600, loss=0.4698645\n",
            "[2018-10-30 05:33:21,562]INFO:root:Epoch: 2, example@2508800, loss=0.5233614\n",
            "[2018-10-30 05:33:24,599]INFO:root:Epoch: 2, example@2560000, loss=0.5172686\n",
            "[2018-10-30 05:33:27,659]INFO:root:Epoch: 2, example@2611200, loss=0.53738594\n",
            "[2018-10-30 05:33:30,704]INFO:root:Epoch: 2, example@2662400, loss=0.50730556\n",
            "[2018-10-30 05:33:33,749]INFO:root:Epoch: 2, example@2713600, loss=0.51373476\n",
            "[2018-10-30 05:33:36,809]INFO:root:Epoch: 2, example@2764800, loss=0.51613116\n",
            "[2018-10-30 05:33:39,858]INFO:root:Epoch: 2, example@2816000, loss=0.5830836\n",
            "[2018-10-30 05:33:42,896]INFO:root:Epoch: 2, example@2867200, loss=0.57232606\n",
            "[2018-10-30 05:33:45,931]INFO:root:Epoch: 2, example@2918400, loss=0.5783198\n",
            "[2018-10-30 05:33:48,976]INFO:root:Epoch: 2, example@2969600, loss=0.5354588\n",
            "[2018-10-30 05:33:52,015]INFO:root:Epoch: 2, example@3020800, loss=0.55398154\n",
            "[2018-10-30 05:33:55,178]INFO:root:\n",
            "Most similar to ['i'] (median: 0.87935): when, all, or, who, into, some, including, most, after, however\n",
            "Most similar to ['may'] (median: 0.78208): can, would, will, all, some, most, for, including, or, i\n",
            "Most similar to ['made'] (median: 0.60151): with, then, called, between, x, through, into, from, over, against\n",
            "Most similar to ['first'] (median: 0.90616): most, through, at, british, when, history, system, other, after, from\n",
            "Most similar to ['would'] (median: 0.77594): can, may, will, could, or, for, after, to, under, into\n",
            "Most similar to ['up'] (median: 0.85843): all, other, through, british, including, or, if, from, when, system\n",
            "Most similar to ['during'] (median: 0.90417): through, at, british, after, into, from, or, most, like, with\n",
            "Most similar to ['or'] (median: 0.92359): after, including, for, british, into, however, under, when, through, all\n",
            "Most similar to ['history'] (median: 0.90391): most, some, when, all, including, many, first, other, who, after\n",
            "Most similar to ['so'] (median: 0.737): both, between, other, had, all, were, new, first, such, their\n",
            "Most similar to ['had'] (median: 0.88119): has, or, were, have, including, after, other, british, for, into\n",
            "Most similar to ['no'] (median: 0.88154): many, all, her, some, its, their, de, his, an, other\n",
            "Most similar to ['m'] (median: 0.91291): isbn, mi, births, strong, figure, km, migrant, purpose, singer, mile\n",
            "Most similar to ['will'] (median: 0.86399): or, most, may, would, can, including, after, like, into, at\n",
            "Most similar to ['being'] (median: 0.84654): including, first, but, out, history, other, many, british, most, all\n",
            "Most similar to ['since'] (median: 0.45936): name, within, same, before, against, national, time, great, with, include\n",
            "[2018-10-30 05:33:55,194]INFO:root:Epoch: 3, example@3072000, loss=0.5319281\n",
            "[2018-10-30 05:33:58,234]INFO:root:Epoch: 3, example@3123200, loss=0.49711573\n",
            "[2018-10-30 05:34:01,256]INFO:root:Epoch: 3, example@3174400, loss=0.5140263\n",
            "[2018-10-30 05:34:04,326]INFO:root:Epoch: 3, example@3225600, loss=0.50169146\n",
            "[2018-10-30 05:34:07,377]INFO:root:Epoch: 3, example@3276800, loss=0.51195014\n",
            "[2018-10-30 05:34:10,432]INFO:root:Epoch: 3, example@3328000, loss=0.5503313\n",
            "[2018-10-30 05:34:13,492]INFO:root:Epoch: 3, example@3379200, loss=0.5237249\n",
            "[2018-10-30 05:34:16,562]INFO:root:Epoch: 3, example@3430400, loss=0.55843383\n",
            "[2018-10-30 05:34:19,641]INFO:root:Epoch: 3, example@3481600, loss=0.53703624\n",
            "[2018-10-30 05:34:22,706]INFO:root:Epoch: 3, example@3532800, loss=0.51184845\n",
            "[2018-10-30 05:34:25,807]INFO:root:Epoch: 3, example@3584000, loss=0.5246228\n",
            "[2018-10-30 05:34:28,838]INFO:root:Epoch: 3, example@3635200, loss=0.49362776\n",
            "[2018-10-30 05:34:31,843]INFO:root:Epoch: 3, example@3686400, loss=0.4373464\n",
            "[2018-10-30 05:34:34,846]INFO:root:Epoch: 3, example@3737600, loss=0.56597036\n",
            "[2018-10-30 05:34:37,849]INFO:root:Epoch: 3, example@3788800, loss=0.52725816\n",
            "[2018-10-30 05:34:40,854]INFO:root:Epoch: 3, example@3840000, loss=0.516832\n",
            "[2018-10-30 05:34:43,876]INFO:root:Epoch: 3, example@3891200, loss=0.49085227\n",
            "[2018-10-30 05:34:46,879]INFO:root:Epoch: 3, example@3942400, loss=0.54612863\n",
            "[2018-10-30 05:34:49,901]INFO:root:Epoch: 3, example@3993600, loss=0.4882063\n",
            "[2018-10-30 05:34:52,898]INFO:root:Epoch: 3, example@4044800, loss=0.49508452\n",
            "[2018-10-30 05:34:56,006]INFO:root:\n",
            "Most similar to ['i'] (median: 0.92697): into, including, being, most, or, after, all, who, however, de\n",
            "Most similar to ['may'] (median: 0.77294): can, would, will, including, most, like, into, three, i, or\n",
            "Most similar to ['made'] (median: 0.68543): then, where, with, called, against, through, while, between, system, from\n",
            "Most similar to ['first'] (median: 0.89937): national, system, at, british, state, through, from, against, most, under\n",
            "Most similar to ['would'] (median: 0.79869): can, may, will, or, to, for, into, after, had, over\n",
            "Most similar to ['up'] (median: 0.80636): against, through, from, like, include, being, on, british, system, into\n",
            "Most similar to ['during'] (median: 0.90742): through, from, at, with, while, on, against, under, british, into\n",
            "Most similar to ['or'] (median: 0.95069): including, after, into, however, but, being, like, all, de, who\n",
            "Most similar to ['history'] (median: 0.86883): most, national, state, system, about, including, first, only, through, large\n",
            "Most similar to ['so'] (median: 0.71451): new, international, other, between, them, her, such, being, both, all\n",
            "Most similar to ['had'] (median: 0.91156): or, have, has, including, after, being, into, like, de, john\n",
            "Most similar to ['no'] (median: 0.90572): many, some, their, all, its, his, an, her, s, de\n",
            "Most similar to ['m'] (median: 0.83147): population, isbn, strong, complete, age, km, william, process, person, type\n",
            "Most similar to ['will'] (median: 0.90245): would, like, into, after, including, most, or, being, against, under\n",
            "Most similar to ['being'] (median: 0.94363): including, but, many, or, de, after, all, however, john, into\n",
            "Most similar to ['since'] (median: 0.68378): during, through, with, from, system, on, against, while, state, at\n",
            "[2018-10-30 05:34:56,020]INFO:root:Epoch: 4, example@4096000, loss=0.5094962\n",
            "[2018-10-30 05:34:59,013]INFO:root:Epoch: 4, example@4147200, loss=0.5345884\n",
            "[2018-10-30 05:35:02,015]INFO:root:Epoch: 4, example@4198400, loss=0.5042243\n",
            "[2018-10-30 05:35:05,023]INFO:root:Epoch: 4, example@4249600, loss=0.5027169\n",
            "[2018-10-30 05:35:08,026]INFO:root:Epoch: 4, example@4300800, loss=0.5564807\n",
            "[2018-10-30 05:35:11,031]INFO:root:Epoch: 4, example@4352000, loss=0.5647713\n",
            "[2018-10-30 05:35:14,036]INFO:root:Epoch: 4, example@4403200, loss=0.5318921\n",
            "[2018-10-30 05:35:17,047]INFO:root:Epoch: 4, example@4454400, loss=0.52343315\n",
            "[2018-10-30 05:35:20,045]INFO:root:Epoch: 4, example@4505600, loss=0.60148144\n",
            "[2018-10-30 05:35:23,043]INFO:root:Epoch: 4, example@4556800, loss=0.48004636\n",
            "[2018-10-30 05:35:26,054]INFO:root:Epoch: 4, example@4608000, loss=0.5274267\n",
            "[2018-10-30 05:35:29,081]INFO:root:Epoch: 4, example@4659200, loss=0.5490308\n",
            "[2018-10-30 05:35:32,192]INFO:root:Epoch: 4, example@4710400, loss=0.4518931\n",
            "[2018-10-30 05:35:35,550]INFO:root:Epoch: 4, example@4761600, loss=0.54703695\n",
            "[2018-10-30 05:35:38,944]INFO:root:Epoch: 4, example@4812800, loss=0.5017538\n",
            "[2018-10-30 05:35:42,312]INFO:root:Epoch: 4, example@4864000, loss=0.5879172\n",
            "[2018-10-30 05:35:45,707]INFO:root:Epoch: 4, example@4915200, loss=0.5424775\n",
            "[2018-10-30 05:35:49,055]INFO:root:Epoch: 4, example@4966400, loss=0.53544337\n",
            "[2018-10-30 05:35:52,399]INFO:root:Epoch: 4, example@5017600, loss=0.5168541\n",
            "[2018-10-30 05:35:55,770]INFO:root:Epoch: 4, example@5068800, loss=0.544258\n",
            "[2018-10-30 05:35:59,221]INFO:root:\n",
            "Most similar to ['i'] (median: 0.94512): some, including, but, or, who, under, however, over, all, like\n",
            "Most similar to ['may'] (median: 0.86222): would, can, will, i, like, including, under, or, over, some\n",
            "Most similar to ['made'] (median: 0.82556): called, where, through, then, into, against, state, by, over, about\n",
            "Most similar to ['first'] (median: 0.93712): national, state, most, through, over, while, from, about, only, against\n",
            "Most similar to ['would'] (median: 0.84609): may, will, can, under, for, like, over, into, against, or\n",
            "Most similar to ['up'] (median: 0.87623): against, include, from, through, being, between, national, like, state, first\n",
            "Most similar to ['during'] (median: 0.88909): from, at, with, british, on, through, system, between, after, against\n",
            "Most similar to ['or'] (median: 0.95172): but, including, under, like, i, however, for, into, who, over\n",
            "Most similar to ['history'] (median: 0.92851): most, state, national, only, first, including, through, out, over, about\n",
            "Most similar to ['so'] (median: 0.818): new, other, international, between, being, first, english, use, national, some\n",
            "Most similar to ['had'] (median: 0.89193): have, has, were, or, like, for, was, will, including, under\n",
            "Most similar to ['no'] (median: 0.924): many, some, their, all, its, several, s, his, other, an\n",
            "Most similar to ['m'] (median: 0.85871): isbn, km, william, strong, births, est, inducted, species, established, bc\n",
            "Most similar to ['will'] (median: 0.92652): would, may, under, like, against, including, into, over, between, or\n",
            "Most similar to ['being'] (median: 0.92228): including, include, against, national, first, other, about, only, but, through\n",
            "Most similar to ['since'] (median: 0.72665): among, during, system, within, government, where, great, from, with, using\n",
            "[2018-10-30 05:35:59,238]INFO:root:Epoch: 5, example@5120000, loss=0.49981004\n",
            "[2018-10-30 05:36:02,578]INFO:root:Epoch: 5, example@5171200, loss=0.4329478\n",
            "[2018-10-30 05:36:05,923]INFO:root:Epoch: 5, example@5222400, loss=0.51961386\n",
            "[2018-10-30 05:36:09,303]INFO:root:Epoch: 5, example@5273600, loss=0.54025555\n",
            "[2018-10-30 05:36:12,646]INFO:root:Epoch: 5, example@5324800, loss=0.47252995\n",
            "[2018-10-30 05:36:15,992]INFO:root:Epoch: 5, example@5376000, loss=0.4963157\n",
            "[2018-10-30 05:36:19,313]INFO:root:Epoch: 5, example@5427200, loss=0.4625105\n",
            "[2018-10-30 05:36:22,688]INFO:root:Epoch: 5, example@5478400, loss=0.5728915\n",
            "[2018-10-30 05:36:26,049]INFO:root:Epoch: 5, example@5529600, loss=0.5361705\n",
            "[2018-10-30 05:36:29,399]INFO:root:Epoch: 5, example@5580800, loss=0.54223126\n",
            "[2018-10-30 05:36:32,743]INFO:root:Epoch: 5, example@5632000, loss=0.5480174\n",
            "[2018-10-30 05:36:36,106]INFO:root:Epoch: 5, example@5683200, loss=0.50707257\n",
            "[2018-10-30 05:36:39,477]INFO:root:Epoch: 5, example@5734400, loss=0.54654056\n",
            "[2018-10-30 05:36:42,834]INFO:root:Epoch: 5, example@5785600, loss=0.50781417\n",
            "[2018-10-30 05:36:46,189]INFO:root:Epoch: 5, example@5836800, loss=0.4981336\n",
            "[2018-10-30 05:36:49,545]INFO:root:Epoch: 5, example@5888000, loss=0.50229716\n",
            "[2018-10-30 05:36:52,869]INFO:root:Epoch: 5, example@5939200, loss=0.5464158\n",
            "[2018-10-30 05:36:56,217]INFO:root:Epoch: 5, example@5990400, loss=0.47690442\n",
            "[2018-10-30 05:36:59,551]INFO:root:Epoch: 5, example@6041600, loss=0.44012907\n",
            "[2018-10-30 05:37:02,651]INFO:root:Epoch: 5, example@6092800, loss=0.5238147\n",
            "[2018-10-30 05:37:05,733]INFO:root:\n",
            "Most similar to ['i'] (median: 0.95429): who, or, when, some, but, english, only, including, all, other\n",
            "Most similar to ['may'] (median: 0.87965): can, would, will, like, i, when, only, or, for, including\n",
            "Most similar to ['made'] (median: 0.90828): against, where, through, state, over, about, while, into, only, most\n",
            "Most similar to ['first'] (median: 0.94029): state, national, most, through, against, system, british, international, over, at\n",
            "Most similar to ['would'] (median: 0.86527): may, will, can, like, after, for, under, against, into, over\n",
            "Most similar to ['up'] (median: 0.88467): against, include, through, being, first, state, from, only, between, including\n",
            "Most similar to ['during'] (median: 0.92182): from, at, with, on, system, british, under, through, between, against\n",
            "Most similar to ['or'] (median: 0.95775): but, when, who, after, i, like, including, for, into, while\n",
            "Most similar to ['history'] (median: 0.89414): out, most, national, state, large, university, first, life, because, including\n",
            "Most similar to ['so'] (median: 0.89383): international, new, other, against, between, people, high, through, including, both\n",
            "Most similar to ['had'] (median: 0.88616): has, have, were, was, after, like, or, will, include, for\n",
            "Most similar to ['no'] (median: 0.93407): many, some, several, its, all, their, her, each, s, other\n",
            "Most similar to ['m'] (median: 0.91438): isbn, km, william, births, inducted, est, p, strong, deaths, age\n",
            "Most similar to ['will'] (median: 0.93319): would, may, like, against, after, including, into, under, through, or\n",
            "Most similar to ['being'] (median: 0.92028): include, against, including, international, first, about, through, state, national, only\n",
            "Most similar to ['since'] (median: 0.75195): among, within, during, government, system, before, great, called, using, usually\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rJBagPrhjVBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3e596833-20df-4daf-df05-d78cfb690311"
      },
      "cell_type": "code",
      "source": [
        "embedding = model.get_layer('embedding')\n",
        "# first set of weights are the learned embedding vectors\n",
        "word_vectors = embedding.get_weights()[0]\n",
        "logging.info('Learned %s %sD vectors', *learned_word_vectors.shape)\n",
        "out = open('data/word2vec.txt', 'w')\n",
        "out.write('{} {}\\n'.format(vocab.size-1, sgns.embedding_dim))\n",
        "for word, idx in vocab.word_2_idx.items():\n",
        "  out.write('{} {}\\n'.format(word, ' '.join(map(str, word_vectors[idx,]))))\n",
        "out.close\n",
        "logging.info('Wrote embedding weights to data/word2vec.txt')"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2018-10-30 07:08:33,881]INFO:root:Learned 10000 300D vectors\n",
            "[2018-10-30 07:08:35,776]INFO:root:Wrote embedding weights to data word2vec.txt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "C_nyIqkdjos1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "1c59237b-631c-42e5-c38d-fbae9b70adf3"
      },
      "cell_type": "code",
      "source": [
        "!head data/word2vec.txt"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9999 300\n",
            "UNK 0.010102738 -0.015697192 0.024387922 4.082918e-05 -0.024800373 -0.014498748 0.037744883 0.007871889 0.011741649 0.039989617 -0.039880253 -0.027034689 0.0076042786 -0.024242878 -0.03025651 -0.014522016 -0.03167983 0.049274985 0.035965648 0.038065765 -0.038012553 -0.022853578 -0.0069702044 0.00016641617 0.03753606 0.021492492 -0.04014889 -0.0032878146 0.031786595 0.016863074 -0.033296444 -0.03644506 0.03576001 -0.0043700114 -0.022072423 -0.048637487 0.0283379 0.004434932 -0.00087534264 -0.022371018 0.0076481104 0.04931618 -0.000258971 0.035994325 0.026989128 0.0037972108 0.03734732 0.018676292 -0.0085910335 -0.024264503 0.016929273 0.035072695 -0.007653117 0.010401774 -0.018887663 0.035085406 0.04949807 -0.033115804 -0.025930513 0.013284732 0.029619489 -0.033128418 0.01675326 0.023451243 0.043576453 -0.01486541 -0.007862259 -0.028250301 0.028645765 -0.04517133 -0.03324919 -0.013556741 -0.01082753 0.04502252 0.016480032 -0.030329837 -0.048045065 -0.009424888 -0.034159996 0.015762854 0.045250956 -0.011875212 -0.031845093 -0.031144524 -0.036736824 -0.04392756 -0.0054545626 -0.029132998 -0.023450112 -0.018421315 -0.02988912 -0.011147607 0.04829185 0.04895706 -0.002500344 -0.00017904118 -0.010467637 -0.025629496 -0.025679518 -0.01087556 -0.02729776 -0.014518429 0.004123997 0.014741469 0.030027185 0.01570237 0.049323346 -0.04865258 0.007995833 -0.020255804 0.009731449 0.010759722 0.019689929 -0.03258715 0.015454654 -0.036530305 -0.022300636 0.04771504 -0.009905718 -0.022622287 0.035768453 0.041847553 0.018941645 0.007550977 -0.0015868545 0.042461563 -0.040270578 0.013480376 0.004636038 0.0071176663 0.043112997 -0.0030752532 0.010665037 0.022275817 -0.0061059 0.030891668 -0.024891198 -0.022609496 0.03035463 -0.04411018 -0.047015157 0.025638964 0.018178608 0.012205802 -0.031229412 0.012103856 -0.04930202 0.040419642 -0.033443592 -0.029356552 0.033004496 0.0067000017 0.048460174 0.032420706 -0.027146721 0.048738886 -0.029505968 0.018211912 -0.040563665 -0.029307975 -0.044612397 -0.018826056 0.03685275 -0.020490421 -0.0018767603 -0.00053204224 0.04137962 0.002966296 -0.018341936 -0.030164469 -0.011402108 0.04689977 -0.025783373 0.034359027 -0.017241716 0.032388177 0.02245916 -0.025101995 0.019728485 -0.04322946 0.038988855 -0.028708829 -0.0136378035 0.023376647 0.008038808 -0.028363204 0.015866067 0.03599802 0.025525037 -0.012392949 0.03407159 -0.027921438 0.03448608 -0.02705505 -0.03803103 -0.035457026 -0.0023867004 -0.030413032 0.009396531 -0.018791044 0.02306011 0.030206468 -0.019973302 0.04463068 -0.0054987893 0.03769629 -6.058067e-05 0.024924245 -0.011288714 -0.013181187 -0.04890087 -0.034833886 0.03705484 0.042364288 0.0019683465 0.018523324 0.047558818 0.012960147 -0.04710442 -0.00084415823 0.031528745 0.045361172 0.013766948 0.049774114 0.021132443 0.019324634 -0.01562643 0.048373807 0.03764147 -0.043308496 0.011630975 0.019794527 -0.03126005 -0.014035713 -0.00028150156 0.0062438473 0.0019765608 -0.030110514 0.0010355003 -0.041383766 -0.0013897903 -0.03304358 0.02036785 -0.04893143 0.026942346 -0.007983398 -0.0069324747 0.0040718205 0.018182423 -0.01112299 -0.012157988 6.7315996e-05 -0.03428799 -0.024818718 -0.022623349 -0.008170389 0.017162573 -0.00601878 0.006210733 -0.025220955 -0.020792639 0.019353118 0.027678933 0.037123848 -0.0374135 0.043361787 -0.04445516 -0.027079677 0.00786835 -0.003563024 -0.0066412464 -0.020177567 0.015724089 0.02731694 0.0009822026 0.022216845 -0.04114436 0.01828884 -0.048761692 0.049989287 0.0057013035 0.018522773 0.03629407 -0.0419968 -0.0101977475 0.024336364 -0.0044331662 0.022398356 -0.048141923 -0.022920776 0.0021222122 -0.019743204 -0.009601962 0.02453946 0.020116579 0.03919569 0.0401459 -0.036572684 0.019711625 -0.03535986\n",
            "the -2.858003 -0.49762306 1.291467 -2.1262543 2.5596817 2.7786045 3.0444517 3.5063686 2.020625 -3.147982 -2.9495156 2.0306823 2.3977168 -3.1940708 3.285859 -3.484772 2.2850308 -2.3295789 3.0727448 1.1630358 -2.7375023 -2.8493505 -3.2454498 2.8094788 -2.5315037 2.8147106 -2.373299 3.1419926 -3.125867 3.2070682 -3.4617116 1.8362249 2.454939 1.7990326 -3.306941 2.450641 3.1620095 -2.160628 -2.2621977 1.5343525 3.3198342 -1.5322155 2.6518536 -2.3969152 3.0333917 -2.6063044 2.656616 3.3842096 -2.9005015 -2.5400794 3.2579691 -0.11213341 -3.1027474 2.9190528 -2.2520258 -2.3608277 -2.64976 2.8024716 -2.3713293 -3.0067418 -3.4968188 2.1569715 2.4896574 -2.507291 1.6720494 -1.6820041 3.0134995 3.1533208 1.2211968 -1.3503206 -1.920319 3.140728 0.22897644 2.8910797 2.828339 3.1181011 1.8026041 -2.9798608 2.884367 -2.6015182 -3.1612163 2.3591812 2.517247 2.6488986 -3.7093334 0.8421857 2.1583943 0.2710249 2.2761238 -1.8352877 2.860026 2.736507 3.1838648 -2.7927027 3.5095468 -1.1948667 -1.3023306 2.5783253 -2.7321718 1.761219 -2.2608404 1.9181275 -3.547832 -3.0006666 3.24376 -2.433862 -3.4779167 1.0042831 3.611292 -3.076497 -1.941934 2.83111 3.2350092 -2.1675906 -2.495079 3.0010302 2.3670032 -2.5046077 -2.9021275 -2.5077903 -3.203353 2.3732917 -3.2101865 1.8341902 3.1163058 2.6558766 -2.2818768 -0.78736967 -0.9243898 2.6242769 0.14791371 3.5902731 -3.0997558 3.5532436 2.3912456 -1.232234 2.7873805 2.9160612 -0.31726813 2.4545379 0.8602755 1.0568885 -2.149455 2.608588 2.300201 -2.6621182 2.6502419 2.3171725 3.3968217 3.2869089 3.4985092 -3.45141 -2.8746836 -2.54761 -1.9155121 2.7841313 2.6834488 2.3879266 1.5761895 -1.8750232 -2.6889796 2.936941 2.408955 3.004244 -1.6305058 2.4058788 2.7036874 -2.8146777 1.6887612 -2.2525911 -2.6234946 -3.144834 -1.9659191 2.8234653 2.888405 -2.3783176 -3.1929588 -3.5081072 -2.9252717 2.2880754 -2.7716005 -2.8830774 -2.0600812 -2.2504106 2.0588295 2.836519 -2.4185524 3.15393 -2.959931 2.6529481 0.06380717 -2.4549687 0.5990669 -3.0011568 -2.5378876 2.1323912 3.3543117 3.578627 2.3735864 -1.9654013 0.1900248 -3.2480834 -3.1869454 -2.7331712 0.22586474 -2.8062034 2.220071 2.0549088 1.5255958 3.4412584 2.8953252 2.5738633 1.4921968 -3.0856655 -2.4071887 2.750471 -3.3556745 2.0323837 2.8040428 -2.7286894 -3.7067788 2.7049634 2.9307966 1.9969274 2.3136601 2.08682 -3.0831993 -2.8942416 -2.7866135 -2.951712 -2.1409686 -2.028554 1.5948586 -3.3674786 -1.9619601 -2.0775151 2.4206092 2.127371 2.9654393 -2.824605 3.198871 2.6509259 2.5727146 -3.116103 -2.0376859 -1.1488532 -1.1262159 -1.1771352 3.5276947 2.6206276 3.0618823 -3.1955347 -3.380599 3.002465 -2.5682116 1.5106977 3.0012674 -3.620839 -0.5789899 2.897766 -1.1580176 2.7337375 3.0221508 -2.2597911 1.9413216 -1.6520466 -3.0977774 2.698747 1.5477672 -2.9618182 0.97370696 2.2703695 1.9568201 -1.9799514 -2.6674225 1.6559821 -0.85561746 2.468504 0.5897879 -3.000433 1.9175142 -3.378915 -3.6057088 1.2081215 -0.9604203 -3.292603 2.7430205 -1.7612382 2.5574934 -2.8993487 3.316611 -3.054187 -2.696793 -2.4759521 1.9580481 -1.1920443 -3.0467668 1.3838618 -1.6782552 -2.7451737\n",
            "of -1.330399 -1.9856591 2.1354303 2.2648585 1.3823317 0.9277139 -2.1316068 -0.4965625 1.8462806 -0.4332899 -1.2051028 1.9216305 0.9430051 -1.0347322 1.0803434 -0.762253 1.2899964 1.2275524 0.96535647 2.3421195 -1.0398284 0.9902784 1.6998457 1.1249652 2.0779858 0.88802236 -1.660559 0.033696484 1.7580255 1.0531188 1.2099445 -2.25495 -2.1546261 2.1528409 1.3107243 1.462743 -0.0158818 2.562043 1.1603211 -2.3097017 0.72736377 1.7777709 1.4258469 -0.7770725 -1.8478497 1.9981264 -2.4360144 1.0065187 -1.4267689 -1.3792754 0.83133304 1.4317361 0.6210455 1.2866194 -1.431961 -1.6741872 -0.5835973 1.5243723 -1.9877015 1.0343707 1.0743967 1.2175835 -2.0280037 2.4580944 1.2626755 -1.9177377 1.4424306 -2.4184318 -1.8280588 -1.6291037 2.528399 -0.8386636 2.0030534 0.9088374 1.0004812 0.7875018 1.5496515 0.13057797 0.9380521 -1.2594974 -1.3266768 -0.84525007 0.66061115 1.3253831 -0.9677067 2.1512365 1.7482722 -2.2767534 1.5788739 2.5947435 -1.6937355 1.183421 0.6320813 -1.3569174 0.3999268 -2.1061776 -2.1589093 1.6753403 -0.27341723 1.920947 -1.7989392 -2.8356986 -0.8615396 -1.3334634 -1.2908888 1.8105344 0.7084584 2.3650198 -1.2808349 0.6036057 2.1913147 1.374872 -1.5928695 -1.484973 -0.3258686 -1.9784913 1.5760579 -1.1258808 -1.122872 2.3612597 -0.036030397 -1.747187 -1.4022973 1.5860641 -0.6179217 1.5264871 1.149201 -2.2250838 2.6316676 1.6566553 -2.4909852 -1.3526465 -1.179383 -1.4343895 1.4423046 2.6287155 -1.9083184 1.5653751 2.6955175 0.8405111 1.7753158 1.9538914 2.2620351 -0.608276 1.5917114 -1.4319198 1.3963333 1.4243809 -1.9528284 0.81224537 -1.877826 -0.87005687 -1.2513475 2.0065324 1.9910609 1.3239717 1.0936836 -0.11275285 -2.6033072 0.5073093 -1.7687349 1.3847402 -1.9386578 -0.95170194 -2.0322819 1.3792503 0.98859155 -1.1675644 -1.8223802 -1.4437233 -1.1795987 -1.1185517 -1.7215092 1.181994 -1.7493244 -1.5053146 -0.74562824 -0.54926974 1.7285615 0.7611997 -1.2673153 -1.0591425 -1.7990602 -0.2821561 1.1217036 -1.9564253 1.4835708 1.3868214 -1.307549 1.3481601 -2.523967 -1.685869 2.370536 2.3499396 1.4872061 0.9640807 -1.8746276 1.0494208 1.1823809 1.8576792 -2.5809426 -0.75700074 -1.3025104 -1.6467061 -2.657187 2.0708275 1.4136592 1.6962365 1.9014052 0.37140313 1.5171782 1.7348049 1.7111255 2.3192823 -1.5590153 1.4222685 -0.8512992 1.729827 -0.24699748 -0.90732324 1.4117383 -0.09067226 1.2003202 -2.496604 1.4630569 1.6765918 1.8081266 -1.4301794 0.18557027 -1.1651028 -1.3148745 -1.6102253 1.653103 1.1581967 2.353177 -1.6272193 1.2573118 1.6506997 -1.8471677 -1.4278537 0.6692023 -2.2638333 -0.89848447 -0.68220615 -1.8666768 -1.976349 2.1857176 -1.7511479 -0.9936956 0.27603307 1.1314185 -0.84930813 -0.8596801 -2.379868 -0.7517215 2.1404707 0.98157185 -0.3198844 -2.1305861 -2.3689692 2.2479262 -0.35689762 0.94223183 -1.3581223 1.922723 -2.0981872 -1.3899801 1.4476303 -2.1228137 0.47752047 2.1822362 1.6318327 1.7517864 2.404486 -0.34388468 0.10540613 -1.6376587 -2.2427318 1.160642 -1.2367928 -0.7343218 1.8537279 -0.6937792 -2.2948656 -2.1678855 1.5501128 1.3989487 0.15895948 0.3355868 -0.27633914 0.79523194 -0.5394044 -1.2433155 -1.51158 1.500989 -1.8025074 0.3241759 2.0020013 -2.0140522 -1.1731327\n",
            "and -1.2954549 -0.9081999 1.5853746 1.174837 1.6226404 1.3350457 -0.17507029 1.2615298 1.6542897 -1.7915597 -1.6536883 1.6372603 1.7438204 -1.7184379 1.5109199 -1.9304979 0.891086 0.7678919 1.6835221 1.500703 -0.90390295 -1.0438641 -0.34858325 1.7955139 0.8117486 1.5155742 -1.3910646 0.15651071 0.4013983 1.865925 -1.5640255 -1.0747069 -0.5665186 1.6531458 -0.44603613 1.5789883 0.8080621 0.75605077 -1.1885071 -1.0977299 1.5519656 0.65947837 1.474927 -1.2237017 -0.9929705 0.634869 -1.1154745 1.0156493 -1.9410619 -1.6308383 1.541015 1.5912836 -1.6664401 1.6272454 -1.4738221 -1.5222557 -1.4208045 1.4365014 -1.2121582 -0.6399382 -1.3445021 1.291094 -0.62865454 0.6829849 1.3329061 -1.5747954 1.6608883 -0.6055975 -1.0825112 -1.0638148 1.0702425 1.5251126 -0.13482408 1.2488182 1.502876 1.6980726 1.3733368 -0.5676682 1.584967 -1.2777737 -1.3938307 0.6372877 1.7403637 1.5191895 -1.7628627 0.8512492 1.7164115 -0.7569809 1.5684096 1.2335211 -0.196634 1.425686 0.954022 -1.7327105 1.3773901 -1.5296197 -1.3425103 1.5218899 -0.2705257 1.4763916 -1.7021228 -1.3614872 -1.7256258 -1.6779286 0.4653173 1.1539849 -1.222399 1.6043004 0.69059914 -0.70762306 1.4381728 1.425098 0.19814062 -1.228576 -1.6499192 -0.6080867 1.6892178 -1.7534142 -1.6112264 0.6572069 -1.613346 -0.83184403 -1.8061626 1.6066519 1.6219671 1.6841974 -0.007268906 -1.5191177 1.8210346 1.6701051 -2.0115335 0.26547518 -1.5631725 0.24122263 1.0476843 1.4051808 -0.5586277 1.7973701 1.5063298 1.182677 0.57086045 1.7272232 0.86080396 1.2055608 1.3504194 -1.4478378 1.4510846 1.0062233 0.44695583 1.4426271 0.46654826 -1.6897904 -1.3834294 0.5313584 1.221023 1.6022741 1.0263821 0.4252409 -0.9914544 0.41564932 -1.4274776 1.8505236 -0.91663706 0.23241742 -1.2393106 1.4227326 1.6074915 -1.6117396 -1.0443443 -1.4098171 -1.7773277 -1.1661556 -1.5118306 1.2177204 -0.6205213 -1.6954879 -1.5607573 -1.5426629 0.9134995 0.4041975 -1.4546878 -1.6550294 -1.4097008 -0.13619128 0.4793559 -0.6856037 -0.08252219 1.4997487 -1.6303904 1.06184 -1.5922639 -1.660282 1.1869409 0.8845655 0.025105776 1.3337746 -0.5170658 1.5747589 0.44897518 0.47212338 -1.3608323 -1.7531627 -1.3216465 -1.5340025 -1.6425884 0.7976224 0.6620342 1.687896 1.5423523 1.4442364 1.5839857 1.6778843 1.7396679 0.96858674 -1.7979453 1.346598 -1.8335634 1.5497811 1.5043714 -1.6899661 -1.2823033 1.8350573 1.7290395 -1.2039511 1.275663 1.7923402 0.86003304 -1.4255304 -1.3241209 -1.6614901 -1.4090754 -1.5281715 0.92557776 -1.1714243 1.0960212 -1.6225145 0.34672132 1.8256353 0.34437513 -1.6029993 1.7554293 -0.8506907 1.0524637 -1.6110796 -1.7356865 -1.6624124 1.4822044 -1.4155809 1.0835437 -0.7027452 1.6389432 -1.1577513 -1.9016057 -1.0208765 -1.6095314 1.2087758 1.2794976 -1.6319313 -1.3190073 -0.8114455 1.0447055 1.3798748 1.7107416 -1.900919 1.3924404 -1.5747387 -1.61746 1.493912 -0.6592073 -1.512013 1.6118618 1.6390754 1.3029126 0.91232324 -1.0611295 1.0900561 -0.4306846 -0.73921025 1.3987232 -1.4903762 1.4465175 0.20821288 -1.6680912 -0.86514896 -1.6440072 -0.13892855 1.7110378 -1.3741835 0.5248922 -1.7346066 1.5731541 -1.329831 -1.5054506 -1.7058299 1.5378486 -1.2595261 -1.8542236 1.4059091 -1.5594802 -1.6450676\n",
            "one -0.44799832 0.20704088 -0.010556208 -0.6430755 1.2498652 0.75666595 -1.476182 0.50509894 1.4828181 -0.2640874 -0.55164415 0.20526461 1.7776438 -0.9945211 0.6867437 -0.73138756 -0.6567276 -0.89376605 0.41296324 1.0276086 -0.45862043 -0.39743236 -0.10589209 0.85012907 1.2678975 -0.09453353 0.24135028 1.9506522 1.036059 1.0118071 -3.1951957 -0.11364683 -0.03597087 0.5605602 -2.6506038 1.5436026 2.3955119 0.04176888 0.32890025 -1.027442 0.2969062 1.5129119 1.7328012 -2.1880486 0.9603653 -0.012264131 -0.05605124 0.108473055 -1.4684429 -0.47291115 0.69005734 0.22949184 -0.14720069 1.318328 -0.3878627 -1.2803429 -0.91316223 0.5546954 -0.5236973 -2.13119 -2.100512 0.0885341 0.16669576 -0.3762106 1.9713974 -1.5022243 1.5975242 0.6191508 -1.691544 0.21754134 0.26147914 0.6239452 -1.4804794 0.5359888 0.30495253 1.2563925 -0.08818337 -1.8376658 0.011359596 -0.6244074 -1.3723261 2.2887917 -0.07246991 2.049017 -1.9389008 -1.231737 0.40354687 -1.4674572 0.6782128 0.037836768 -1.2650988 1.284246 -0.75910276 -1.0477176 0.9495892 -0.8346674 -0.53331786 0.9853374 1.6947234 0.26942933 -0.8934814 -1.234476 -0.773965 -1.3074492 0.7138465 -0.50383264 -2.1172779 0.4085415 0.34507447 -0.5767791 1.126541 1.9260764 -0.8349075 -0.8348696 -0.9869338 1.2353568 0.8775445 -0.9556245 -0.8470554 0.94055194 -1.8750541 1.5985518 -0.68353444 0.07709356 1.7422789 0.44415948 0.30647045 -0.09423814 0.0507165 0.33498612 -0.4850247 1.9466964 -0.48069158 1.8349828 -0.80111784 0.07543453 1.6328102 1.0673616 -0.42937577 0.9125067 0.17831714 1.3453598 0.010078741 0.48550454 0.3377226 -1.5894358 0.96821743 0.68906194 1.7933878 0.21176508 2.4562306 -1.0747427 -0.62092304 0.20486398 1.2491658 1.1608545 0.7078151 1.0561227 -0.26377925 -0.39430156 -0.579943 1.0646939 -1.1456726 1.6081865 -0.32206064 0.44081476 0.7750467 -0.77954715 1.1595007 -0.26273128 -1.7678704 -0.6487932 0.025609624 0.44642434 0.11356391 -0.34980398 -0.826363 -1.5161355 1.187305 1.713063 -1.2516056 -2.0697987 -0.31290248 -0.38144726 -1.9350662 1.3346545 -0.085698344 1.3686835 -0.7934699 1.0952857 -0.24039401 -1.5450034 0.90624917 -0.1419177 -0.7546514 0.11796906 -1.2522709 1.3349359 -0.5401605 0.39612898 -1.027883 -0.22202745 -0.77264845 -0.56336117 -0.67699933 -0.1700125 1.341332 0.76713914 1.240177 2.4983015 0.7524268 0.74516165 0.3767558 -0.13115749 -0.49588975 0.6942443 -0.88731295 0.28956833 1.581411 -2.0724776 -2.339679 2.228759 0.7629923 -0.17604604 1.5274873 0.45714524 -0.013387384 -0.22630218 0.4152063 -1.8702381 -0.44223872 -0.840767 0.7740034 -1.5890636 0.09130279 -2.1277049 0.02740537 1.6647481 1.3736275 -0.9254972 1.3623779 0.07087546 -0.48953828 -1.178327 -1.6336435 -0.6781854 -0.12844405 -0.26871207 -0.112446256 -0.48974 1.5990778 -0.5293377 -0.8897055 2.0664353 -1.9434873 1.582941 0.3829335 -2.0114706 -0.16166984 -2.125046 1.0655304 1.4626986 0.3541855 -1.195309 1.0085268 -0.27186498 -1.6493542 1.123716 -0.0182048 -0.4535692 0.31727406 0.5798737 1.6743181 0.6895149 -0.40898558 0.72288823 -1.5471163 -0.3486099 1.269782 -0.3034897 0.11104232 0.7950335 -0.46732506 -0.6506071 -0.11484969 -1.1307807 1.3918052 0.094440795 1.7911701 -1.7836272 1.0342908 -0.94875026 -0.44510284 -1.1880136 1.1941931 -1.3853617 -0.34768555 0.5692013 -0.21028455 -1.5189353\n",
            "in -0.89198047 -1.5802879 1.2240711 1.610029 1.1144352 0.5954525 -1.2689092 0.58036125 0.9384201 -0.9185451 -1.2575239 1.3605634 0.22641538 -1.0167143 0.9945607 -0.95240754 0.6121056 1.3385692 1.1483555 1.1204176 -0.95193094 -0.2686023 1.4121045 1.1116356 1.5326767 1.0421029 -1.2538205 -0.61865985 1.4306166 0.7833106 -0.6270816 -1.5367892 -1.4854589 1.3214608 1.0675311 0.91496974 -0.21472894 1.7170231 -0.9513045 -1.7853633 0.6348134 0.5336338 0.8838598 -0.90230894 -1.8341179 1.1877514 -1.4769275 0.55051637 -0.9507003 -1.05093 1.0291864 1.7949138 -0.64222854 0.12441598 -1.042789 -1.2123128 -0.11766518 1.2014174 -0.40381238 -0.4671923 0.47404715 1.2625785 -1.4009037 1.6334491 1.201765 -0.97424066 0.77706784 -1.4549184 -1.0611722 -1.5243363 1.555091 0.48971546 0.45044005 0.036168993 0.87550074 0.9195586 1.1727617 0.32776245 0.6734708 -0.3204858 -0.7402567 0.32665658 1.4045655 -0.11858988 -0.52905333 1.6498328 1.2792398 -1.4826087 1.4150134 1.5560813 0.11704233 0.96168137 0.42552042 -1.0178071 0.82483447 -1.2580386 -1.6001925 0.99416167 -0.09625406 1.4752861 -1.2294983 -1.6438069 -0.50675225 -0.7035913 -0.8523605 1.3108279 0.28697023 1.2954674 -0.656422 -0.25698978 1.5155756 0.39453754 -1.0464268 -0.91958106 -0.8270922 -1.2097821 1.2200795 -0.90650916 -0.28337145 1.7334017 -0.51605016 -1.758596 -0.8248412 1.3990151 0.48825642 1.0404667 0.28690597 -1.7958648 2.005496 1.2736106 -1.647796 -0.5838088 -1.0643605 -1.1244884 0.7609967 1.3418317 -1.5376521 0.88799906 1.9268967 1.0426879 1.3366293 1.5271772 1.5185826 0.74333906 1.2492746 -0.95886385 1.166435 0.92023605 -1.1102308 0.89584684 -1.2683123 -0.93080497 -0.7402638 1.3586701 1.5254891 0.77766514 0.05420323 0.48003095 -1.7822467 1.3677535 -0.9454752 0.3824024 -1.2784126 -1.3570329 -1.351803 1.1673342 0.9004033 -1.066673 -1.6345786 -1.3630263 -0.4953294 -0.37241727 -1.5515344 0.94578505 -1.1887114 -1.4581827 -0.6926322 -0.82249576 1.7297126 -0.42600834 -1.0431559 -0.67501175 -1.5475311 -0.8445932 0.38431925 -1.8582734 0.80868876 0.63177466 -0.9447172 0.82352453 -1.7769215 -0.90110075 1.4047401 1.7886456 0.89101183 1.7049931 -1.4844627 0.4722588 0.43781567 1.3143553 -1.6665266 0.014149982 -0.9816031 -1.0406642 -1.5555233 1.3099203 0.9197497 1.3079588 1.192216 0.065300815 0.7839455 1.1615598 1.5633905 1.3272741 -1.3381127 0.45945278 -0.98346496 1.3255694 0.9637394 -0.52763706 0.04218676 6.988151e-05 0.14479688 -1.5186305 0.8908672 1.1787446 1.5268412 -1.2113707 -1.1199592 -0.7762447 -1.0043114 -1.2809771 1.063248 0.5831773 1.6464105 -0.7685365 0.66753936 1.0653149 -1.3158463 -1.1343883 0.861477 -1.8594905 0.7856087 -0.7269116 -1.2028944 -1.5554861 1.8073541 -1.1226838 0.4365613 -0.27964208 0.9110259 -0.99720067 -0.322999 -1.5597672 -0.7425225 0.99743587 -0.043923337 -0.0031412565 -1.6922612 -1.4668903 1.4188696 0.69248784 0.92361575 -0.8851377 1.1226263 -1.2582848 -0.8117007 -0.028562095 -1.7764348 -0.8087816 1.7965953 1.1856104 0.83434874 1.4406211 -0.8860198 1.4665984 -1.2405646 -1.6624972 1.5213804 -1.0949216 0.6344397 1.1242287 -0.09630029 -1.3874476 -1.6889263 1.4165375 0.99239695 -1.1677526 -0.89695823 -0.91456413 0.29793003 -0.9227257 -1.123651 -0.8390511 0.8996693 -1.4112712 -0.88161856 1.5172102 -1.2225035 -0.9034575\n",
            "a -1.464681 -0.4837472 0.015750855 -0.9626585 1.4332495 2.0731952 0.9537928 2.0645747 1.4040776 -1.9148685 -1.6571568 0.8810134 1.0873755 -1.9148711 1.8183032 -2.0476985 1.5584646 -0.2156104 1.7882483 0.67287534 -1.6703453 -1.9936634 -1.1997076 1.7847753 -0.9060974 1.6684524 -1.3872895 2.2342908 -1.2475169 1.8608245 -2.018452 0.5688646 1.1625861 1.0863652 -1.4539677 1.1060845 2.2917147 -0.822029 -1.3727756 -0.014853185 1.7685028 0.51520634 1.556308 -0.9976387 1.6703708 -0.974253 1.2791325 1.8786542 -1.3383527 -1.1600573 1.8796135 0.11685785 -1.5913105 1.8284715 -1.3575336 -1.4080802 -1.8829229 1.4281569 -1.3391021 -2.172994 -2.130534 1.0016255 0.7024383 -1.1245946 1.0369582 -0.59332585 1.5330037 1.5716443 -0.42036143 -1.0671943 -0.30150795 1.8710856 -0.009226023 1.9357522 1.5571654 1.5328337 1.1717782 -1.9367893 1.7828255 -1.540831 -1.671813 1.6471893 0.9276304 1.7244419 -2.1436443 1.0701395 1.2960327 -0.3259758 1.0903949 -0.06449879 2.2201943 1.7841744 2.0887918 -1.5561886 2.0296545 -0.98086995 -0.778695 1.4952801 -0.030343309 0.7785949 -1.2776729 1.1845936 -2.2375588 -1.6686858 2.325354 -0.7418099 -2.0680163 1.040119 2.2963827 -2.5016568 -0.038005743 1.7871276 2.4062505 -1.3925805 -1.7484269 1.2162995 0.96904224 -0.8996538 -1.8989766 -0.49194926 -2.1205847 0.5366027 -1.9329954 0.8003439 1.5633813 1.4088566 -1.6953344 -0.3071215 -0.5936284 1.4421098 -0.16816075 1.0300868 -1.4124262 1.0957435 1.5366219 -0.026576592 1.6187968 1.4801921 -0.009417888 1.4446911 1.1025873 0.53866494 -0.22859934 2.022525 1.0734547 -1.8022314 1.4615443 1.2060612 0.879001 1.9201331 1.8225182 -1.6050833 -1.4577973 -0.61026734 -0.6361662 1.5129126 2.1331801 1.9412794 0.66065633 -0.48753044 -1.4652125 1.5656804 0.6343775 1.1606202 -1.0063425 1.6241176 1.5262566 -1.6956167 0.051897068 -1.149074 -1.523735 -2.1395192 -1.0909969 1.8880807 0.8384184 -1.1790997 -1.6137705 -2.0662751 -1.3979822 1.8468515 -1.6481626 -1.5374209 -1.2495353 -1.3477199 0.80355614 1.527232 -0.3029248 1.9691554 -1.6324445 1.5630633 -0.2636057 -1.2942103 0.5981797 -1.0836719 -0.42283398 1.1439531 1.1222708 1.9803486 1.3135765 0.05706964 -0.54934466 -1.6915879 -1.7169641 -1.4576422 -0.5369155 -0.82259256 1.7224646 0.9952122 0.9393115 2.0946224 1.6505482 1.2088641 0.86042166 -1.6990564 -1.1464913 1.5629284 -1.9181993 1.2672753 1.909394 -1.6202563 -2.4381537 2.0466459 1.9807229 0.88051033 1.1613144 1.3779385 -1.4054825 -1.4760273 -1.9660819 -1.7385864 -1.2393316 -0.51931995 1.2644029 -2.172427 -0.28012198 -1.6911601 1.6925414 1.1686181 2.1419127 -1.1946441 1.9856577 0.16292931 0.9748778 -1.5965574 -1.2485036 -0.7296849 0.07513459 -1.2027003 2.3169658 0.50129455 1.7651393 -1.9352378 -2.1722424 1.896494 -1.5810668 0.74366194 1.8122182 -2.0181367 -0.5793125 1.8585911 0.2164876 1.5287834 1.7798606 -0.9778156 1.0123516 -1.0163338 -1.7368709 1.4232877 0.040864255 -1.9217395 0.6543508 1.3288614 1.1359882 -0.41271338 -1.6941981 1.1934658 -0.63737667 0.79294556 0.12970415 -1.414674 1.595211 -1.6276213 -1.7203914 0.536781 -0.6920241 -1.0661051 1.5539418 -1.241318 0.14845964 -1.4965519 1.9388521 -1.9978347 -1.2865605 -1.3493217 0.7999284 -0.8280076 -1.8578713 0.85956126 -1.117761 -1.790487\n",
            "to -0.37436476 -1.4235054 1.5927203 2.1478572 1.4839121 -0.30865034 -1.4525932 0.8193829 1.1717689 0.18092154 -1.3905256 0.9855196 0.20476532 -0.07488719 1.1468836 -0.8307557 0.23310773 -0.02388425 0.9775963 1.4284494 -1.2364911 0.7290717 1.1614858 1.3810335 1.4867163 -0.08173137 -1.4223917 -0.8169222 -0.22384165 1.2489522 -0.96273816 -1.8023392 -1.7027786 1.3610986 -0.9534564 1.5020876 0.637115 1.8828148 -0.5109298 -1.2175802 1.0322934 1.2753043 0.8001865 -0.105955474 -1.8956443 0.51546067 -1.7083331 0.7243529 -1.1738116 -1.2639353 1.1276977 2.0289555 -1.2582331 0.93317574 -1.2978219 -1.42013 0.5179428 1.3569053 -1.4788859 -0.48133144 -0.53906536 1.2558173 -1.3542788 1.5530871 1.5885835 -1.2778449 1.3322268 -1.8401496 -1.3503709 -1.612875 1.5843717 -0.32380626 1.6766747 -0.4680188 0.05574323 1.1763223 1.6684403 0.19250703 1.4232167 -1.0616534 -1.0808274 -0.39464855 1.2176414 1.5270832 -1.0828183 2.0466876 1.6704707 -1.4413757 1.6534579 1.8657609 -1.3212247 1.0777348 0.9821234 -1.260968 -0.066613585 -1.5621407 -1.5736506 1.5163677 1.0526648 1.7171353 -1.2391381 -1.4000745 0.25544897 -1.3404754 -1.3015628 1.8524445 -0.6105562 1.941648 -1.2875319 0.72505873 1.81938 1.320443 0.3838451 -0.29861093 -1.1664336 -0.3503576 1.0848132 -0.7582755 -0.3101038 1.792518 0.5471513 -1.9223645 -1.0918285 1.2860665 1.2353904 1.1324694 -0.0028913217 -1.4336954 1.9907655 1.1419894 -1.6918803 0.30103916 -0.8878065 -1.9346935 0.78051084 1.9502897 -0.74343675 1.036797 2.102699 1.2032574 1.3270923 1.686378 1.2669332 -0.20496818 1.2263554 -0.9762147 1.5521237 0.3195066 -1.7961998 0.04187781 -1.4212658 -1.1085036 -0.8438004 1.4705195 1.4557883 1.0816495 -0.4203827 0.446352 -1.3599467 1.3821838 -1.3785993 1.168804 -1.6486562 -1.5551944 -1.2743291 1.6214786 -0.07853941 -0.48720017 -0.085967444 -1.0967853 -0.761703 -0.6751575 -1.490426 1.2299659 0.43576145 -1.4526641 0.28626153 -1.0189582 1.5299315 -0.2656484 -1.046279 -0.607681 -1.4284649 0.60830855 -1.1160475 -2.2016854 1.3600638 0.9223421 -0.45530918 0.72577894 -1.9427973 -0.9056188 1.2347186 1.6730359 -0.12751658 0.6891937 -0.40228644 0.38333076 -0.3980185 1.3119841 -1.7238872 0.46164128 -0.92833763 -0.45211837 -2.0512147 0.8326682 1.1949441 1.56391 1.5968893 0.518995 1.2055185 1.2587892 1.7574146 1.6966377 -1.6900033 1.1471736 -0.6984308 1.8055896 0.922625 -0.040474333 0.9898999 -0.66050315 1.0023217 -1.3983607 0.024128178 0.5509956 -0.106163844 -1.1535729 -1.2707754 -1.4029351 0.108899534 -1.5815468 0.981609 0.8177428 1.2221426 -0.8561142 1.0749564 1.5716844 -1.1875718 -0.9494043 0.6014883 -1.8155445 0.5794783 0.29013184 -1.5057654 -1.5995536 1.9084623 -1.641151 0.51779103 0.3225144 0.5555602 -1.0160041 -0.66411334 -2.2974334 0.12243445 0.9947864 0.6497338 -0.400419 -1.9756905 -2.413413 1.7028346 0.8872411 -0.29731277 -1.436045 1.1709923 -1.5422845 -1.0929763 1.05184 -1.2988412 -1.2563776 1.6437056 1.2920803 1.4098383 1.2463084 -0.76739395 1.6453372 0.09926965 -1.6672723 0.7811268 -1.0794129 1.4820033 1.4921701 0.0118944375 -1.1700037 -2.0102255 1.5275779 1.1653613 -1.3213089 0.41921863 -1.1382983 -0.14182815 -0.2571895 -1.5192688 -0.43669495 1.0898411 -1.1232796 -1.0318005 1.6635916 -1.6839328 -1.0539992\n",
            "zero -0.4253394 -0.5089821 0.6826408 0.49402025 0.2996806 0.5741304 -0.16666362 0.51504254 0.11328272 -0.40446928 -0.4744657 0.5240954 0.49240476 -0.4385516 0.5124568 -0.3641447 0.41744405 0.310753 0.45880026 0.2720123 -0.35780257 -0.42473406 0.47438312 0.45539153 0.36829457 0.27946287 -0.37030455 0.22270823 0.3637618 0.46309724 0.059453093 -0.4736971 -0.37535042 0.44397286 -0.06376863 0.43622392 0.3901688 0.5035388 -0.36019593 -0.3285001 0.40226972 0.2219022 0.41449666 -0.3264285 -0.19815865 0.45302916 -0.30585307 0.5947284 -0.40974605 -0.29353732 0.5916164 0.43684813 -0.6018096 0.48782995 -0.54381627 -0.47964954 -0.40746853 0.45985198 -0.5611305 -0.26993245 -0.2827364 0.48564887 -0.37343493 0.45500436 0.24923566 -0.4405199 0.3973639 -0.52728206 -0.17200266 -0.55866987 0.4484429 0.40810525 0.33645678 0.38416946 0.15264738 0.38579252 0.2506131 -0.37690097 0.500495 -0.50247556 -0.45033938 0.17264289 0.37161335 0.37807432 -0.34992337 0.61117744 0.4841028 -0.33212206 0.52119434 0.46057695 0.47046688 0.20596457 0.5435619 -0.3836592 0.46372336 -0.36718398 -0.51038617 0.48058388 0.32376453 0.53736967 -0.38557 -0.26562983 -0.27171662 -0.20237504 0.45980203 0.31342417 -0.23553598 0.45533964 0.419027 -0.21678048 0.25595084 0.36058405 0.24886645 -0.48075292 -0.34453878 -0.32061785 0.3816838 -0.38254255 -0.38630277 0.23015587 -0.24347982 -0.37548298 -0.54138845 0.55942076 0.3793827 0.5504843 -0.41565567 -0.47719336 0.20522372 0.48791447 -0.2749076 -0.5696131 -0.5227776 -0.13568434 0.678212 0.49391684 -0.32951993 0.46303153 0.34256425 0.5244173 0.08574315 0.35490513 0.45473754 0.47619003 0.434249 -0.1543305 0.46129465 0.4433272 -0.36646402 0.5013469 -0.32645205 -0.4307792 -0.6258074 0.4045115 0.3812786 0.46838754 0.2839437 0.23322304 -0.5039647 0.46803755 -0.4655692 0.49973688 -0.4024452 -0.35839322 -0.47396076 0.36856622 0.4562399 -0.5458768 -0.34865716 -0.3975435 -0.39582384 -0.48629415 -0.5153969 0.388512 -0.38891226 -0.35786128 -0.5158477 -0.40948483 -0.20945576 0.3136612 -0.432365 -0.30258664 -0.41910905 -0.6607744 -0.1364622 -0.44387853 0.20953599 0.38950008 -0.33884123 0.46449816 -0.31816116 -0.4332504 0.5743457 0.21381608 0.33275938 0.4634112 -0.39633796 0.5509095 0.43104973 0.47750828 -0.31594893 -0.6850087 -0.47432864 -0.40322316 -0.39299238 0.09566753 0.4507947 0.40150952 0.48673567 0.29020968 0.4836162 0.4326446 0.33897334 0.4530698 -0.44210744 0.5356733 -0.38047075 0.6095908 0.3249782 -0.35467535 0.075495996 0.21753511 0.58372617 -0.3738543 0.4325486 0.45855534 0.45727244 -0.4826562 -0.35688365 -0.4310058 -0.30883896 -0.3188619 0.28080046 -0.3904304 0.22097373 -0.3834691 0.36823735 0.18324134 0.52737254 -0.592093 0.30540228 -0.17693521 0.32573265 -0.3550145 -0.37467432 -0.54036665 0.20509489 -0.36998963 0.5047039 -0.31174812 0.22485639 -0.34418437 -0.4887349 -0.5956152 -0.24188569 0.3425728 0.40195027 -0.3719206 -0.38345572 -0.15355839 0.3714309 0.41468388 0.4901271 -0.26424184 0.45687222 -0.38378125 -0.37259316 0.37532818 -0.4934136 -0.43828824 0.51869947 0.5007815 0.48200884 0.43932942 -0.46008113 0.39947549 -0.32741082 -0.33755612 0.33418417 -0.41202077 0.43125373 0.26721302 -0.6370205 -0.4000899 -0.2662829 0.33538544 0.3795913 -0.22262329 -0.46593204 -0.27969489 0.59026647 -0.4637651 -0.49397957 -0.42077368 0.31751597 -0.4107973 -0.52885073 0.32707703 -0.45232075 -0.37542805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GmmKE9amzlnL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7ef5007c-1059-4bea-b76c-492f897b1c04"
      },
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"shailesh.kochhar@gmail.com\"\n",
        "!git config --global user.name \"Shailesh Kochhar\"\n",
        "!git add data/word2vec.txt\n",
        "!git commit -m \"updated weights\""
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[master 180848f] updated weights\n",
            " 1 file changed, 10000 insertions(+)\n",
            " create mode 100644 data/word2vec.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DcUEQWzc0ELx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "5fade99b-e851-462a-ea56-82e6b73bd7a0"
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Your branch and 'origin/master' have diverged,\n",
            "and have 1 and 1 different commits each, respectively.\n",
            "  (use \"git pull\" to merge the remote branch into yours)\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add/rm <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\t\u001b[31mdeleted:    Keras-Word2Vec.ipynb\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31mKeras_Word2Vec.ipynb\u001b[m\n",
            "\t\u001b[31mword2vec/__pycache__/\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}